{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.cluster import MeanShift, MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Books.csv does not exist.\n"
     ]
    }
   ],
   "source": [
    "# List of CSV file names\n",
    "csv_files = ['Books.csv']\n",
    "\n",
    "# Function to read and clean CSV files\n",
    "def clean_csv_files(file_list):\n",
    "    for file in file_list:\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file):\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(file)\n",
    "            # Drop rows with any null values\n",
    "            df_cleaned = df.dropna()\n",
    "            # Create a new file name with '_cleaned' appended\n",
    "            new_file_name = os.path.splitext(file)[0] + '_cleaned.csv'\n",
    "            # Save the cleaned dataframe to a new CSV file\n",
    "            df_cleaned.to_csv(new_file_name, index=False)\n",
    "            print(f'Cleaned {file} and saved the new file as {new_file_name}.')\n",
    "        else:\n",
    "            print(f'File {file} does not exist.')\n",
    "\n",
    "# Call the function\n",
    "clean_csv_files(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File books.csv does not exist.\n"
     ]
    }
   ],
   "source": [
    "def clean_and_trim_csv_files(file_list):\n",
    "    for file in file_list:\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file):\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(file)\n",
    "            # Drop the last 2 columns\n",
    "            df_trimmed = df.iloc[:, :-6]\n",
    "            # Drop rows with any null values\n",
    "            df_cleaned = df_trimmed.dropna()\n",
    "            # Create a new file name with '_cleaned' appended\n",
    "            new_file_name = os.path.splitext(file)[0] + '_cleaned.csv'\n",
    "            # Save the cleaned dataframe to a new CSV file\n",
    "            df_cleaned.to_csv(new_file_name, index=False)\n",
    "            print(f'Processed {file}, dropped the last 2 columns, removed nulls, and saved as {new_file_name}.')\n",
    "        else:\n",
    "            print(f'File {file} does not exist.')\n",
    "\n",
    "# List of CSV file names\n",
    "csv_files = ['books.csv']\n",
    "\n",
    "# Call the new function\n",
    "clean_and_trim_csv_files(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Temp\\ipykernel_7712\\2049580182.py:5: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  books_df = pd.read_csv(books_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-matching ISBNs:\n",
      "                 ISBN      _merge\n",
      "0          904492401X   left_only\n",
      "1         #069580216X   left_only\n",
      "2            #6612432   left_only\n",
      "3       (THEWINDMILLP   left_only\n",
      "4          )416195113   left_only\n",
      "...               ...         ...\n",
      "341233     1874166633  right_only\n",
      "341234      130897930  right_only\n",
      "341235     020130998X  right_only\n",
      "341236     2268032019  right_only\n",
      "341237     3442150663  right_only\n",
      "\n",
      "[71071 rows x 2 columns]\n",
      "          ISBN  Book-Rating  \\\n",
      "52  000104687X          6.0   \n",
      "53  000104799X          7.5   \n",
      "56  000123207X          0.0   \n",
      "57  000160418X          3.5   \n",
      "59  000171421X          0.0   \n",
      "\n",
      "                                           Book-Title     Book-Author  \\\n",
      "52  T.S. Eliot Reading \\The Wasteland\\\" and Other ...      T.S. Eliot   \n",
      "53                                        Monk's-hood    Ellis Peters   \n",
      "56                        Paddington's Birthday Party    Michael Bond   \n",
      "57                     The Clue in the Crumbling Wall   Carolyn Keene   \n",
      "59      It's Not Easy Being a Bunny (A Beginner Book)  Marilyn Sadler   \n",
      "\n",
      "   Year-Of-Publication                 Publisher  \n",
      "52                1993  HarperCollins Publishers  \n",
      "53                1994  HarperCollins Publishers  \n",
      "56                1942  HarperCollins Publishers  \n",
      "57                1984  HarperCollins Publishers  \n",
      "59                1984  HarperCollins Publishers  \n"
     ]
    }
   ],
   "source": [
    "# Function to read CSV, compute average ratings, merge datasets and identify non-matching ISBNs\n",
    "def combine_datasets(ratings_file, books_file):\n",
    "    # Read the datasets\n",
    "    ratings_df = pd.read_csv(ratings_file)\n",
    "    books_df = pd.read_csv(books_file)\n",
    "    \n",
    "    # Convert ISBNs to strings to ensure matching works correctly\n",
    "    ratings_df['ISBN'] = ratings_df['ISBN'].astype(str)\n",
    "    books_df['ISBN'] = books_df['ISBN'].astype(str)\n",
    "    \n",
    "    # Group the ratings by ISBN and calculate the average rating\n",
    "    average_ratings = ratings_df.groupby('ISBN')['Book-Rating'].mean().reset_index()\n",
    "    \n",
    "    # Merge the datasets on ISBN\n",
    "    merged_df = pd.merge(average_ratings, books_df, on='ISBN', how='outer', indicator=True)\n",
    "    \n",
    "    # Separate out the non-matching entries\n",
    "    non_matching = merged_df[merged_df['_merge'] != 'both']\n",
    "    \n",
    "    # Check and print non-matching ISBNs\n",
    "    if not non_matching.empty:\n",
    "        print(\"Non-matching ISBNs:\")\n",
    "        print(non_matching[['ISBN', '_merge']])\n",
    "    else:\n",
    "        print(\"All ISBNs match between the datasets.\")\n",
    "    \n",
    "    # Return only the rows that matched both\n",
    "    return merged_df[merged_df['_merge'] == 'both'].drop(columns=['_merge'])\n",
    "\n",
    "# Replace 'ratings.csv' and 'books.csv' with your actual file paths\n",
    "combined_df = combine_datasets(r'C:\\Users\\tyler\\OneDrive\\Desktop\\GitHub Repositories\\DataScience2\\Exit_Tickets\\March28th\\CleanedData\\ratings_cleaned.csv', r'C:\\Users\\tyler\\OneDrive\\Desktop\\GitHub Repositories\\DataScience2\\Exit_Tickets\\March28th\\CleanedData\\books_cleaned.csv')\n",
    "\n",
    "# You can now work with 'combined_df' which contains the merged data\n",
    "print(combined_df.head())  # This line is just to show the first few entries of the combined dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Temp\\ipykernel_7712\\137031853.py:4: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  books_df = pd.read_csv(books_file)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>000104687X</td>\n",
       "      <td>6.0</td>\n",
       "      <td>T.S. Eliot Reading \\The Wasteland\\\" and Other ...</td>\n",
       "      <td>T.S. Eliot</td>\n",
       "      <td>1993</td>\n",
       "      <td>HarperCollins Publishers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>000104799X</td>\n",
       "      <td>7.5</td>\n",
       "      <td>Monk's-hood</td>\n",
       "      <td>Ellis Peters</td>\n",
       "      <td>1994</td>\n",
       "      <td>HarperCollins Publishers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>000123207X</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Paddington's Birthday Party</td>\n",
       "      <td>Michael Bond</td>\n",
       "      <td>1942</td>\n",
       "      <td>HarperCollins Publishers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>000160418X</td>\n",
       "      <td>3.5</td>\n",
       "      <td>The Clue in the Crumbling Wall</td>\n",
       "      <td>Carolyn Keene</td>\n",
       "      <td>1984</td>\n",
       "      <td>HarperCollins Publishers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>000171421X</td>\n",
       "      <td>0.0</td>\n",
       "      <td>It's Not Easy Being a Bunny (A Beginner Book)</td>\n",
       "      <td>Marilyn Sadler</td>\n",
       "      <td>1984</td>\n",
       "      <td>HarperCollins Publishers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ISBN  Book-Rating  \\\n",
       "52  000104687X          6.0   \n",
       "53  000104799X          7.5   \n",
       "56  000123207X          0.0   \n",
       "57  000160418X          3.5   \n",
       "59  000171421X          0.0   \n",
       "\n",
       "                                           Book-Title     Book-Author  \\\n",
       "52  T.S. Eliot Reading \\The Wasteland\\\" and Other ...      T.S. Eliot   \n",
       "53                                        Monk's-hood    Ellis Peters   \n",
       "56                        Paddington's Birthday Party    Michael Bond   \n",
       "57                     The Clue in the Crumbling Wall   Carolyn Keene   \n",
       "59      It's Not Easy Being a Bunny (A Beginner Book)  Marilyn Sadler   \n",
       "\n",
       "   Year-Of-Publication                 Publisher  \n",
       "52                1993  HarperCollins Publishers  \n",
       "53                1994  HarperCollins Publishers  \n",
       "56                1942  HarperCollins Publishers  \n",
       "57                1984  HarperCollins Publishers  \n",
       "59                1984  HarperCollins Publishers  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_datasets(ratings_file, books_file):\n",
    "    # Read the datasets\n",
    "    ratings_df = pd.read_csv(ratings_file)\n",
    "    books_df = pd.read_csv(books_file)\n",
    "    \n",
    "    # Convert ISBNs to strings\n",
    "    ratings_df['ISBN'] = ratings_df['ISBN'].astype(str)\n",
    "    books_df['ISBN'] = books_df['ISBN'].astype(str)\n",
    "    \n",
    "    # Group the ratings by ISBN and calculate the average rating\n",
    "    average_ratings = ratings_df.groupby('ISBN')['Book-Rating'].mean().reset_index()\n",
    "    \n",
    "    # Merge the datasets on ISBN with an indicator\n",
    "    merged_df = pd.merge(average_ratings, books_df, on='ISBN', how='outer', indicator=True)\n",
    "    \n",
    "    # Remove entries that didn't match in the books dataset ('left_only')\n",
    "    clean_merged_df = merged_df[merged_df['_merge'] == 'both'].drop(columns=['_merge'])\n",
    "    \n",
    "    # Return the cleaned merged DataFrame\n",
    "    return clean_merged_df\n",
    "\n",
    "# Use the function to clean the datasets\n",
    "# Replace 'ratings.csv' and 'books.csv' with the actual paths to your files\n",
    "cleaned_combined_df = clean_datasets(r'C:\\Users\\tyler\\OneDrive\\Desktop\\GitHub Repositories\\DataScience2\\Exit_Tickets\\March28th\\CleanedData\\ratings_cleaned.csv', r'C:\\Users\\tyler\\OneDrive\\Desktop\\GitHub Repositories\\DataScience2\\Exit_Tickets\\March28th\\CleanedData\\books_cleaned.csv')\n",
    "\n",
    "# Display the first few entries of the cleaned dataframe\n",
    "cleaned_combined_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=90, Silhouette Score: 0.22021024048372773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=91, Silhouette Score: 0.214690987911686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=92, Silhouette Score: 0.23035606059065655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=93, Silhouette Score: 0.22694288431073495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=94, Silhouette Score: 0.22846118190586465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=95, Silhouette Score: 0.2103333118314087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=96, Silhouette Score: 0.23218120735215092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=97, Silhouette Score: 0.22708722072705761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=98, Silhouette Score: 0.25052361397558853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=99, Silhouette Score: 0.2348922042450446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=100, Silhouette Score: 0.23357479730997444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=101, Silhouette Score: 0.22495934858776614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=102, Silhouette Score: 0.22900658896584314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=103, Silhouette Score: 0.23512125141789286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=104, Silhouette Score: 0.248977360404804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=105, Silhouette Score: 0.2570457705990885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=106, Silhouette Score: 0.247633490065075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=107, Silhouette Score: 0.2555457305231726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=108, Silhouette Score: 0.24463619929197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=109, Silhouette Score: 0.2537133089449231\n",
      "Best k value: 105 with a silhouette score of 0.2570457705990885\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to sample a percentage of the data\n",
    "def sample_data(df, percentage=50):\n",
    "    return df.sample(frac=percentage/100)\n",
    "\n",
    "# Set the percentage of data to use\n",
    "data_percentage = 10  # Change this value between 0 and 100 as needed\n",
    "\n",
    "# Sample the data\n",
    "sampled_df = sample_data(cleaned_combined_df, percentage=data_percentage)\n",
    "\n",
    "# Preprocessing steps\n",
    "sampled_df['Year-Of-Publication'] = pd.to_numeric(sampled_df['Year-Of-Publication'], errors='coerce')\n",
    "sampled_df = sampled_df.dropna(subset=['Year-Of-Publication'])\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(sampled_df[['Year-Of-Publication', 'Book-Rating']])\n",
    "hasher = FeatureHasher(n_features=100, input_type='string')\n",
    "hashed_features = hasher.transform(sampled_df['Book-Author'].apply(lambda x: [x]).tolist()).toarray()\n",
    "X = np.hstack((scaled_features, hashed_features))\n",
    "\n",
    "# Range of k values to test\n",
    "k_values = range(90, 110)  # Test from 2 to 9 clusters\n",
    "\n",
    "# Dictionary to store silhouette scores for each k\n",
    "silhouette_scores = {}\n",
    "\n",
    "for k in k_values:\n",
    "    mbk = MiniBatchKMeans(n_clusters=k, batch_size=100, verbose=0)\n",
    "    mbk_labels = mbk.fit_predict(X)\n",
    "    silhouette_score_k = silhouette_score(X, mbk_labels)\n",
    "    silhouette_scores[k] = silhouette_score_k\n",
    "    print(f'k={k}, Silhouette Score: {silhouette_score_k}')\n",
    "\n",
    "# Output the best k value\n",
    "best_k = max(silhouette_scores, key=silhouette_scores.get)\n",
    "print(f'Best k value: {best_k} with a silhouette score of {silhouette_scores[best_k]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=2, Silhouette Score: 0.7636202888934444\n",
      "Best k value: 2 with a silhouette score of 0.7636202888934444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to sample a percentage of the data\n",
    "def sample_data(df, percentage=50):\n",
    "    return df.sample(frac=percentage/100)\n",
    "\n",
    "# Set the percentage of data to use\n",
    "data_percentage = 10  # Change this value between 0 and 100 as needed\n",
    "\n",
    "# Sample the data\n",
    "sampled_df = sample_data(cleaned_combined_df, percentage=data_percentage)\n",
    "\n",
    "# Preprocessing steps\n",
    "sampled_df['Year-Of-Publication'] = pd.to_numeric(sampled_df['Year-Of-Publication'], errors='coerce')\n",
    "sampled_df = sampled_df.dropna(subset=['Year-Of-Publication'])\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(sampled_df[['Year-Of-Publication', 'Book-Rating']])\n",
    "hasher = FeatureHasher(n_features=100, input_type='string')\n",
    "hashed_features = hasher.transform(sampled_df['Book-Author'].apply(lambda x: [x]).tolist()).toarray()\n",
    "X = np.hstack((scaled_features, hashed_features))\n",
    "\n",
    "# Range of k values to test\n",
    "k_values = range(2,3)  # Test from 2 to 9 clusters\n",
    "\n",
    "# Dictionary to store silhouette scores for each k\n",
    "silhouette_scores = {}\n",
    "\n",
    "for k in k_values:\n",
    "    mbk = MiniBatchKMeans(n_clusters=k, batch_size=100, verbose=0)\n",
    "    mbk_labels = mbk.fit_predict(X)\n",
    "    silhouette_score_k = silhouette_score(X, mbk_labels)\n",
    "    silhouette_scores[k] = silhouette_score_k\n",
    "    print(f'k={k}, Silhouette Score: {silhouette_score_k}')\n",
    "\n",
    "# Output the best k value\n",
    "best_k = max(silhouette_scores, key=silhouette_scores.get)\n",
    "print(f'Best k value: {best_k} with a silhouette score of {silhouette_scores[best_k]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Shift clustering in progress...\n",
      "Clustering complete.\n",
      "Mean Shift Silhouette Score: 0.7625203770280946\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to sample a percentage of the data\n",
    "def sample_data(df, percentage=50):\n",
    "    return df.sample(frac=percentage/100)\n",
    "\n",
    "# Set the percentage of data to use\n",
    "data_percentage = 5  # Change this value between 0 and 100 as needed\n",
    "\n",
    "# Sample the data\n",
    "sampled_df = sample_data(cleaned_combined_df, percentage=data_percentage)\n",
    "\n",
    "# Preprocessing steps\n",
    "sampled_df['Year-Of-Publication'] = pd.to_numeric(sampled_df['Year-Of-Publication'], errors='coerce')\n",
    "sampled_df = sampled_df.dropna(subset=['Year-Of-Publication'])\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(sampled_df[['Year-Of-Publication', 'Book-Rating']])\n",
    "hasher = FeatureHasher(n_features=100, input_type='string')\n",
    "hashed_features = hasher.transform(sampled_df['Book-Author'].apply(lambda x: [x]).tolist()).toarray()\n",
    "X = np.hstack((scaled_features, hashed_features))\n",
    "\n",
    "# Mean Shift clustering\n",
    "ms = MeanShift(bin_seeding=True, n_jobs=-1)  # bin_seeding and n_jobs for faster computation\n",
    "print(\"Mean Shift clustering in progress...\")\n",
    "ms_labels = ms.fit_predict(X)\n",
    "print(\"Clustering complete.\")\n",
    "\n",
    "# Silhouette score\n",
    "ms_silhouette = silhouette_score(X, ms_labels)\n",
    "print('Mean Shift Silhouette Score:', ms_silhouette)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters found by Mean Shift: 2\n"
     ]
    }
   ],
   "source": [
    "# Extracting the cluster labels\n",
    "labels = ms.labels_\n",
    "\n",
    "# Calculating the number of clusters\n",
    "n_clusters_ = len(np.unique(labels))\n",
    "\n",
    "print(f\"Number of clusters found by Mean Shift: {n_clusters_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Bandwidth: 1.5880321856815443\n",
      "Number of clusters found by Mean Shift after adjustment: 2\n"
     ]
    }
   ],
   "source": [
    "# Estimating the bandwidth on a sample of the data\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)\n",
    "\n",
    "# Use the estimated bandwidth to fit the Mean Shift model\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "ms.fit(X)\n",
    "\n",
    "# Extracting the new cluster labels and counting clusters\n",
    "labels = ms.labels_\n",
    "n_clusters_ = len(np.unique(labels))\n",
    "\n",
    "print(f\"Estimated Bandwidth: {bandwidth}\")\n",
    "print(f\"Number of clusters found by Mean Shift after adjustment: {n_clusters_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical Clustering Silhouette Score: 0.7597989338949945\n"
     ]
    }
   ],
   "source": [
    "# Function to sample a percentage of the data\n",
    "def sample_data(df, percentage=50):\n",
    "    return df.sample(frac=percentage/100, random_state=42)\n",
    "\n",
    "# Set the percentage of data to use\n",
    "data_percentage = 10  # Change this value between 0 and 100 as needed\n",
    "\n",
    "# Assuming 'cleaned_combined_df' is your DataFrame\n",
    "# Sample a percentage of the data\n",
    "sampled_df = sample_data(cleaned_combined_df, percentage=data_percentage)\n",
    "\n",
    "# Convert 'Year-Of-Publication' to numeric and handle possible non-numeric entries\n",
    "sampled_df['Year-Of-Publication'] = pd.to_numeric(sampled_df['Year-Of-Publication'], errors='coerce')\n",
    "sampled_df = sampled_df.dropna(subset=['Year-Of-Publication'])\n",
    "\n",
    "# Scale the numeric features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(sampled_df[['Year-Of-Publication', 'Book-Rating']])\n",
    "\n",
    "# Feature hashing for 'Book-Author'\n",
    "hasher = FeatureHasher(n_features=100, input_type='string')\n",
    "hashed_features = hasher.transform(sampled_df['Book-Author'].apply(lambda x: [x]).tolist()).toarray()\n",
    "\n",
    "# Combine scaled and hashed features\n",
    "X = np.hstack((scaled_features, hashed_features))\n",
    "\n",
    "# Hierarchical Clustering\n",
    "n_clusters = 2  # Adjust the number of clusters as needed\n",
    "hclust = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "hclust_labels = hclust.fit_predict(X)\n",
    "\n",
    "# Silhouette score\n",
    "hclust_silhouette = silhouette_score(X, hclust_labels)\n",
    "print('Hierarchical Clustering Silhouette Score:', hclust_silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book for User 805: X/1999-Duet   (Book 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "books_df = pd.read_csv(r'C:\\Users\\tyler\\OneDrive\\Desktop\\GitHub Repositories\\DataScience2\\Exit_Tickets\\March28th\\CleanedData\\books_cleaned.csv', dtype={'Year-Of-Publication': str})\n",
    "ratings_df = pd.read_csv(r'C:\\Users\\tyler\\OneDrive\\Desktop\\GitHub Repositories\\DataScience2\\Exit_Tickets\\March28th\\CleanedData\\ratings_cleaned.csv')\n",
    "\n",
    "# Find the books that user 805 has already read\n",
    "user_805_read = ratings_df[ratings_df['User-ID'] == 805]['ISBN'].unique()\n",
    "\n",
    "# Filter these books from the main books dataset\n",
    "books_to_recommend = books_df[~books_df['ISBN'].isin(user_805_read)]\n",
    "\n",
    "# Recommend a book with the highest rating\n",
    "# First, merge the books dataframe with the ratings dataframe to get average ratings\n",
    "merged_books_ratings = pd.merge(books_to_recommend, ratings_df, on='ISBN')\n",
    "average_ratings = merged_books_ratings.groupby('ISBN')['Book-Rating'].mean().reset_index()\n",
    "\n",
    "# Merge back with books dataframe to get book titles\n",
    "recommended_books = pd.merge(average_ratings, books_to_recommend, on='ISBN')\n",
    "\n",
    "# Get the highest-rated book\n",
    "highest_rated_book = recommended_books.sort_values('Book-Rating', ascending=False).head(1)\n",
    "\n",
    "# Output the recommendation\n",
    "if not highest_rated_book.empty:\n",
    "    book_title = highest_rated_book['Book-Title'].values[0]\n",
    "    print(f\"Recommended Book for User 805: {book_title}\")\n",
    "else:\n",
    "    print(\"No book found to recommend.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
